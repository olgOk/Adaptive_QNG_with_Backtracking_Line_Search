{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLemfMqt5pMwSAUBXWU1jt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olgOk/Adaptive_QNG/blob/main/code/NaturalGradientWithLineSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y-Xfx0LmjzJ",
        "outputId": "ce92646e-88fe-4249-a0ac-9ff0f7505dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.3/241.3 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.5/37.5 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for qiskit (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install -q qiskit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Iterable\n",
        "from typing import List, Tuple, Callable, Optional, Union\n",
        "import functools\n",
        "import numpy as np\n",
        "\n",
        "from qiskit.circuit.quantumcircuit import _compare_parameters\n",
        "from qiskit.circuit import ParameterVector, ParameterExpression\n",
        "from qiskit.utils import optionals as _optionals\n",
        "\n",
        "from qiskit.opflow.operator_base import OperatorBase\n",
        "from qiskit.opflow.list_ops.list_op import ListOp\n",
        "from qiskit.opflow.list_ops.composed_op import ComposedOp\n",
        "from qiskit.opflow.state_fns.circuit_state_fn import CircuitStateFn\n",
        "from qiskit.opflow.gradients.circuit_gradients import CircuitGradient\n",
        "from qiskit.opflow.gradients.circuit_qfis import CircuitQFI\n",
        "from qiskit.opflow.gradients.gradient import Gradient\n",
        "from qiskit.opflow.gradients.gradient_base import GradientBase\n",
        "from qiskit.opflow.gradients.qfi import QFI\n",
        "\n",
        "# Error tolerance variable\n",
        "ETOL = 1e-8\n",
        "# Cut-off ratio for small singular values for least square solver\n",
        "RCOND = 1e-2"
      ],
      "metadata": {
        "id": "U3qgUls4nA_k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LineSearchNaturalGradient(GradientBase):\n",
        "  \"\"\"Quantum Natural Gradient with Line Search.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      grad_method: Union[str, CircuitGradient] = \"lin_comb\",\n",
        "      qfi_method: Union[str, CircuitQFI] = \"lin_comb_full\",\n",
        "      **kwargs,\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      grad_method: The method used to compute the state gradient. Can be either\n",
        "                  ``'param_shift'`` or ``'lin_comb'`` or ``'fin_diff'``.\n",
        "      qfi_method: The method used to compute the QFI. Can be either\n",
        "                  ``'lin_comb_full'`` or ``'overlap_block_diag'`` or ``'overlap_diag'``.\n",
        "    \"\"\"\n",
        "    super().__init__(grad_method)\n",
        "\n",
        "    self._qfi_method = QFI(qfi_method)\n",
        "    # is this the tolerance parameter?\n",
        "    self._epsilon = kwargs.get(\"epsilon\", 1e-3) \n",
        "\n",
        "  def convert(\n",
        "      self,\n",
        "      operator: OperatorBase,\n",
        "      params: Optional[\n",
        "          Union[ParameterVector, ParameterExpression, List[ParameterExpression]]\n",
        "        ] = None,\n",
        "    ) -> OperatorBase:\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        operator: The operator we are taking the gradient of.\n",
        "        params: The parameters we are taking the gradient with respect to. If not explicitly\n",
        "                passed, they are inferred from the operator and sorted by name.\n",
        "      Returns:\n",
        "        An operator whose evaluation yields the NaturalGradient.\n",
        "      Raises:\n",
        "        TypeError: If ``operator`` does not represent an expectation value or the quantum\n",
        "                state is not ``CircuitStateFn``.\n",
        "          ValueError: If ``params`` contains a parameter not present in ``operator``.\n",
        "          ValueError: If ``operator`` is not parameterized.\n",
        "      \"\"\"\n",
        "      if not isinstance(operator, ComposedOp):\n",
        "        if not (isinstance(operator, ListOp) and len(operator.oplist) == 1):\n",
        "          raise TypeError(\n",
        "            \"Please provide the operator either as ComposedOp or as ListOp of \"\n",
        "            \"a CircuitStateFn potentially with a combo function.\"\n",
        "          )\n",
        "\n",
        "      if not isinstance(operator[-1], CircuitStateFn):\n",
        "        raise TypeError(\n",
        "          \"Please make sure that the operator for which you want to compute \"\n",
        "          \"Quantum Fisher Information represents an expectation value or a \"\n",
        "          \"loss function and that the quantum state is given as \"\n",
        "          \"CircuitStateFn.\"\n",
        "          )\n",
        "      if len(operator.parameters) == 0:\n",
        "          raise ValueError(\"The operator we are taking the gradient of is not parameterized!\")\n",
        "      if params is None:\n",
        "          params = sorted(operator.parameters, key=functools.cmp_to_key(_compare_parameters))\n",
        "      if not isinstance(params, Iterable):\n",
        "          params = [params]\n",
        "      # Instantiate the gradient\n",
        "      grad = Gradient(self._grad_method, epsilon=self._epsilon).convert(operator, params)\n",
        "\n",
        "      # Instantiate the QFI metric which is used to re-scale the gradient\n",
        "      metric = self._qfi_method.convert(operator[-1], params) * 0.25\n",
        "\n",
        "      def combo_fn(x):\n",
        "        print(x)\n",
        "        return self.nat_grad_combo_fn(x)\n",
        "\n",
        "      # Define the ListOp which combines the gradient and the QFI according to the combination\n",
        "      # function defined above.\n",
        "      return ListOp([grad, metric], combo_fn=combo_fn)\n",
        "\n",
        " \n",
        "  def nat_grad_combo_fn(x) -> np.ndarray:\n",
        "      \"\"\"\n",
        "      Natural Gradient Function Implementation.\n",
        "      Args:\n",
        "        x: Iterable consisting of Gradient, Quantum Fisher Information.\n",
        "            regularization: Regularization method.\n",
        "      Returns:\n",
        "        Natural Gradient.\n",
        "      Raises:\n",
        "          ValueError: If the gradient has imaginary components that are non-negligible.\n",
        "      \"\"\"\n",
        "      gradient = x[0]\n",
        "      metric = x[1]\n",
        "\n",
        "      if np.amax(np.abs(np.imag(gradient))) > ETOL:\n",
        "        raise ValueError(\n",
        "          \"The imaginary part of the gradient are non-negligible. The largest absolute \"\n",
        "          f\"imaginary value in the gradient is {np.amax(np.abs(np.imag(gradient)))}. \"\n",
        "          \"Please increase the number of shots.\"\n",
        "            )\n",
        "        gradient = np.real(gradient)\n",
        "\n",
        "        if np.amax(np.abs(np.imag(metric))) > ETOL:\n",
        "            raise ValueError(\n",
        "                \"The imaginary part of the metric are non-negligible. The largest \"\n",
        "                \"absolute imaginary value in the gradient is \"\n",
        "                f\"{np.amax(np.abs(np.imag(metric)))}. Please \"\n",
        "                \"increase the number of shots.\"\n",
        "            )\n",
        "        metric = np.real(metric)\n",
        "\n",
        "        # Check if numerical instabilities lead to a metric which is not positive semidefinite\n",
        "        w, v = np.linalg.eigh(metric)\n",
        "\n",
        "        if not all(ew >= (-1) * ETOL for ew in w):\n",
        "          raise ValueError(\n",
        "                    f\"The underlying metric has at least one Eigenvalue < -{ETOL}. \"\n",
        "                    f\"The smallest Eigenvalue is {np.amin(w)} \"\n",
        "                    \"Please use a regularized least-square solver for this problem or \"\n",
        "                    \"increase the number of backend shots.\",\n",
        "                    )\n",
        "        if not all(ew >= 0 for ew in w):\n",
        "          # If not all eigenvalues are non-negative, set them to a small positive\n",
        "          # value\n",
        "          w = [max(ETOL, ew) for ew in w]\n",
        "          # Recompose the adapted eigenvalues with the eigenvectors to get a new metric\n",
        "          metric = np.real(v @ np.diag(w) @ np.linalg.inv(v))\n",
        "      \n",
        "      nat_grad = LineSearchNaturalGradient._line_search_solver(metric, gradient)\n",
        "\n",
        "      nat_grad = np.linalg.lstsq(metric, gradient, rcond=RCOND)[0]\n",
        "      return nat_grad\n",
        "\n",
        "  \n",
        "  def _line_search_solver(\n",
        "        metric: np.ndarray,\n",
        "        gradient: np.ndarray,\n",
        "    )-> np.ndarray:\n",
        "\n",
        "    from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "    params = {\n",
        "    \"n_estimators\": 500,\n",
        "    \"max_depth\": 4,\n",
        "    \"min_samples_split\": 5,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"loss\": \"squared_error\"}\n",
        "\n",
        "    reg = ensemble.GradientBoostingRegressor(**params)\n",
        "\n",
        "    def reg_method(a, c, alpha):\n",
        "            reg.set_params(alpha=alpha)\n",
        "            if normalize:\n",
        "                reg.fit(StandardScaler().fit_transform(a), c)\n",
        "            else:\n",
        "                reg.fit(a, c)\n",
        "            return reg.coef_\n",
        "\n",
        "    something = reg_method(metric, gradient, alpha=0.1)\n",
        "\n",
        "    print(something)\n",
        "\n",
        "\n",
        "  def back_tracking_lin_search(cal_obj, cal_obj_grad, param, beta, alpha, max_iter=10):\n",
        "    \"\"\"Implementation of the Backtracking Line Search with Armijo's rule.\n",
        "    \"\"\"\n",
        "    \n",
        "    grad = cal_obj_grad(param)\n",
        "    obj = cal_obj(param)\n",
        "    \n",
        "    for i in range(max_iter):\n",
        "        tmp_beta = beta/2**i\n",
        "        tmp_param = param - tmp_beta*grad\n",
        "        tmp_obj = cal_obj(tmp_param)\n",
        "        \n",
        "        armijo_cond = alpha*tmp_beta*np.linalg.norm(grad)**2\n",
        "        \n",
        "        print(tmp_beta, tmp_param, obj, tmp_obj, armijo_cond)\n",
        "        \n",
        "        if abs(obj - tmp_obj) >= armijo_cond:\n",
        "            return tmp_param\n",
        "        \n",
        "    return tmp_param\n",
        "            \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jWTH7FENqtC1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit.opflow import X, Z, I, StateFn, CircuitStateFn, SummedOp\n",
        "from qiskit.circuit.library import RealAmplitudes\n",
        "from qiskit.circuit import QuantumCircuit, QuantumRegister, Parameter, ParameterVector, ParameterExpression\n",
        "\n",
        "H_model = (0.4 * I ^ Z) + (0.4 * Z ^ I) + (0.2 * X ^ X)\n",
        "\n",
        "# constract the ansatz\n",
        "theta1 = Parameter('theta1')\n",
        "theta2 = Parameter('theta2')\n",
        "q = QuantumRegister(2)\n",
        "qc = QuantumCircuit(q)\n",
        "qc.ry(theta1, q[0])\n",
        "qc.ry(theta2, q[1])\n",
        "qc.cx(q[0],q[1])\n",
        "params = [theta1, theta2]\n",
        "\n",
        "op = ~StateFn(H_model) @ CircuitStateFn(primitive=qc, coeff=1.)\n",
        "\n",
        "# define and compute the QFI\n",
        "state = CircuitStateFn(primitive=qc, coeff=1.)\n",
        "qfi = QFI(qfi_method='lin_comb_full').convert(operator=state, params=params)\n",
        "\n",
        "values_dict = {theta1:-0.1, theta2:-0.2}\n",
        "\n",
        "# Assign the parameters and evaluate the QFI\n",
        "qfi_result = qfi.assign_parameters(values_dict).eval()\n",
        "# print('full QFI \\n', 0.25*np.real(np.array(qfi_result)))"
      ],
      "metadata": {
        "id": "s9B-lHZMpuXJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nat_grad = LineSearchNaturalGradient(grad_method='lin_comb', qfi_method='lin_comb_full').convert(operator=op, params=params)\n",
        "\n",
        "# Assign the parameters and evaluate the gradient\n",
        "nat_grad_result = nat_grad.assign_parameters(values_dict).eval()\n",
        "print('Natural gradient computed with linear combination of unitaries', nat_grad_result)"
      ],
      "metadata": {
        "id": "4HZIbR9Jpz8X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}